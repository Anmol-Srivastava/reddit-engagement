{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('../../reddit-engagement-files/activity.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''SELECT * FROM activity;''')\n",
    "results = cursor.fetchall()\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP For Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>access_time</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l9c06q</td>\n",
       "      <td>Avergage people are just as greedy as rich peo...</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l9bayq</td>\n",
       "      <td>Obese people infuriate me.</td>\n",
       "      <td>TrueOffMyChest</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l90oq6</td>\n",
       "      <td>I am proud to do my part in paying forward our...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l8xsfa</td>\n",
       "      <td>Cool, but why would they bother telling us?</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kzul59</td>\n",
       "      <td>There is absolutely no goddamn reason why you ...</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kwf3ub</td>\n",
       "      <td>Apple launches major new Racial Equity and Jus...</td>\n",
       "      <td>apple</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kurmrg</td>\n",
       "      <td>I wonder how many pros consider GAing pros?</td>\n",
       "      <td>CoDCompetitive</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ktf66r</td>\n",
       "      <td>Latinx is bullshit</td>\n",
       "      <td>TrueOffMyChest</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>krk1c0</td>\n",
       "      <td>Why is it a taboo to criticize cultures that d...</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>krhja1</td>\n",
       "      <td>CMV: The white teen who said the n word on a S...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>01/31/2021 20:39:56</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  l9c06q  Avergage people are just as greedy as rich peo...   \n",
       "1  l9bayq                         Obese people infuriate me.   \n",
       "2  l90oq6  I am proud to do my part in paying forward our...   \n",
       "3  l8xsfa        Cool, but why would they bother telling us?   \n",
       "4  kzul59  There is absolutely no goddamn reason why you ...   \n",
       "5  kwf3ub  Apple launches major new Racial Equity and Jus...   \n",
       "6  kurmrg        I wonder how many pros consider GAing pros?   \n",
       "7  ktf66r                                 Latinx is bullshit   \n",
       "8  krk1c0  Why is it a taboo to criticize cultures that d...   \n",
       "9  krhja1  CMV: The white teen who said the n word on a S...   \n",
       "\n",
       "          subreddit          access_time action  \n",
       "0  unpopularopinion  01/31/2021 20:39:56   down  \n",
       "1    TrueOffMyChest  01/31/2021 20:39:56   down  \n",
       "2    wallstreetbets  01/31/2021 20:39:56   down  \n",
       "3    wallstreetbets  01/31/2021 20:39:56   down  \n",
       "4  unpopularopinion  01/31/2021 20:39:56   down  \n",
       "5             apple  01/31/2021 20:39:56   down  \n",
       "6    CoDCompetitive  01/31/2021 20:39:56   down  \n",
       "7    TrueOffMyChest  01/31/2021 20:39:56   down  \n",
       "8    TooAfraidToAsk  01/31/2021 20:39:56   down  \n",
       "9      changemyview  01/31/2021 20:39:56   down  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql_query('SELECT * FROM activity;', conn)\n",
    "print(len(df))\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>access_time</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>ldemd0</td>\n",
       "      <td>Nope, not in the great US of A!</td>\n",
       "      <td>WhitePeopleTwitter</td>\n",
       "      <td>02/05/2021 21:00:04</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>ldekzg</td>\n",
       "      <td>Steph and Ayesha Curry Have Quietly Served Up ...</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>02/05/2021 21:00:04</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>ldbhtp</td>\n",
       "      <td>But but but the trickle</td>\n",
       "      <td>PoliticalHumor</td>\n",
       "      <td>02/05/2021 21:00:04</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>ld8c35</td>\n",
       "      <td>Remaster of the Mass Effect games changes some...</td>\n",
       "      <td>SubredditDrama</td>\n",
       "      <td>02/05/2021 21:00:04</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "1539  ldemd0                    Nope, not in the great US of A!   \n",
       "1540  ldekzg  Steph and Ayesha Curry Have Quietly Served Up ...   \n",
       "1541  ldbhtp                            But but but the trickle   \n",
       "1542  ld8c35  Remaster of the Mass Effect games changes some...   \n",
       "\n",
       "               subreddit          access_time action  \n",
       "1539  WhitePeopleTwitter  02/05/2021 21:00:04     up  \n",
       "1540       UpliftingNews  02/05/2021 21:00:04     up  \n",
       "1541      PoliticalHumor  02/05/2021 21:00:04     up  \n",
       "1542      SubredditDrama  02/05/2021 21:00:04     up  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = df.tail(n=10).copy()\n",
    "c[c.subreddit.apply(lambda x: x.lower() not in ['codcompetitive'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anmol/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anmol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/anmol/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor here\n",
    "def preprocess_text(text):\n",
    "    # remove urls/numbers\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    # expand contractions\n",
    "    text = re.sub(r\"'ve'\", ' have', text)\n",
    "    text = re.sub(r\"n't\", ' not', text)\n",
    "    # remove emojis and special characters\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub('[^a-zA-Z0-9]+', ' ', text)\n",
    "    # remove stop words\n",
    "    stops = stopwords.words('english')\n",
    "    text = ' '.join([x for x in text.split() if x not in stops])\n",
    "    # tokenize\n",
    "    text = word_tokenize(text)\n",
    "    lm = WordNetLemmatizer()\n",
    "    text = [lm.lemmatize(x) for x in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heard\n",
      "would\n"
     ]
    }
   ],
   "source": [
    "for word in preprocess_text(\"haven't heard of wouldn't\"):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heard', 'poop']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
